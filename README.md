# GAFTA Guardian

Production-ready LightRAG system for legal document analysis using GPU-accelerated Llama 3.3 70B.

## ğŸš€ Quick Start

```bash
# 1. Clone repository
git clone <repo-url> gafta-guardian
cd gafta-guardian

# 2. Setup NVIDIA driver (if needed)
sudo bash fix_nvidia_driver.sh
sudo reboot

# 3. Pull models (40GB+ - takes time!)
bash setup_models.sh

# 4. Start services
docker compose up -d --build

# 5. Verify
curl http://localhost:8000/health
```

## ğŸ“‹ Requirements

- **Server**: Ubuntu 22.04+ with NVIDIA GPU
- **GPU**: 80GB+ VRAM (H100 tested)
- **NVIDIA Driver**: 575+
- **Docker**: 20.10+ with GPU support
- **Storage**: 50GB+ for models

## ğŸ“ Project Structure

```
gafta-guardian/
â”œâ”€â”€ lightrag_api/          # FastAPI application
â”‚   â”œâ”€â”€ main.py           # API endpoints
â”‚   â”œâ”€â”€ Dockerfile        # Container definition
â”‚   â”œâ”€â”€ requirements.txt  # Python dependencies
â”‚   â””â”€â”€ static/           # Web interface
â”œâ”€â”€ docker-compose.yml    # Service orchestration
â”œâ”€â”€ setup_models.sh       # Model download script
â”œâ”€â”€ fix_nvidia_driver.sh  # Driver fix script
â”œâ”€â”€ *.py                  # Utility scripts
â””â”€â”€ *.md                  # Documentation
```

## ğŸ”§ Configuration

### Environment Variables

- `OLLAMA_BASE_URL`: Ollama service URL (default: `http://ollama:11434`)
- `LLM_MODEL`: LLM model name (default: `llama3.3:70b`)
- `EMBEDDING_MODEL`: Embedding model (default: `bge-m3`)

### Ports

- **8000**: FastAPI/LightRAG API
- **11434**: Ollama API

## ğŸ“– Documentation

- **[SETUP_GUIDE.md](SETUP_GUIDE.md)**: Complete setup instructions
- **[ARCHITECTURE.md](ARCHITECTURE.md)**: System architecture details
- **[DEPLOYMENT.md](DEPLOYMENT.md)**: Deployment checklist

## ğŸ¯ Usage

### Ingest Documents

```bash
curl -X POST http://localhost:8000/ingest \
  -H "Content-Type: application/json" \
  -d '{"text": "Document content...", "doc_id": "doc_001"}'
```

### Query Knowledge Graph

```bash
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"query": "Is Company X in defaulters?", "mode": "hybrid"}'
```

### Web Interface

Open `http://YOUR_SERVER_IP:8000` in browser

## âš ï¸ Known Issues

1. **Query Returns Null**: Async function compatibility issue with LightRAG wrapper. Documents are ingested successfully, but queries may return null until resolved.

2. **Model Download**: Llama 3.3 70B is 40GB+ and takes significant time to download.

## ğŸ’¾ Backup & Restore

### Backup

```bash
# Models (40GB+)
tar -czf ollama_data_backup.tar.gz ollama_data/

# Knowledge graph
tar -czf rag_data_backup.tar.gz rag_data/
```

### Restore

```bash
tar -xzf ollama_data_backup.tar.gz
tar -xzf rag_data_backup.tar.gz
docker compose up -d
```

## ğŸ”’ Security Notes

- No authentication implemented (add for production)
- Exposed ports: 8000, 11434 (use firewall)
- All processing is local (no external API calls)

## ğŸ’° Cost Optimization

- **Stop when not in use**: `docker compose down`
- **Models persist**: No need to re-download
- **Graph persists**: No need to re-ingest

## ğŸ› Troubleshooting

### GPU Not Working
```bash
nvidia-smi
docker run --rm --gpus all nvidia/cuda:11.0.3-base-ubuntu20.04 nvidia-smi
```

### Services Not Starting
```bash
docker compose logs -f
docker compose ps
```

### Health Check Fails
```bash
curl http://localhost:11434/api/tags  # Check Ollama
curl http://localhost:8000/health     # Check API
```

## ğŸ“ Development Status

- âœ… LightRAG initialization: **Fixed**
- âœ… Document ingestion: **Working**
- âš ï¸ Query endpoint: **Returns null** (async function issue)
- âœ… GPU acceleration: **Working**
- âœ… Model persistence: **Working**
- âœ… Knowledge graph persistence: **Working**

## ğŸ”® Next Steps

1. Fix async function compatibility
2. Add authentication
3. Implement query caching
4. Add batch processing
5. Performance optimization

## ğŸ“„ License

[Your License Here]

## ğŸ™ Acknowledgments

- [LightRAG](https://github.com/HKUDS/LightRAG) - Graph-based RAG system
- [Ollama](https://ollama.ai/) - Local LLM server
- [FastAPI](https://fastapi.tiangolo.com/) - Web framework
